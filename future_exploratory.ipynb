{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29050eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a11f9a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64117/544167729.py:3: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  y = pd.read_csv(data, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F14</th>\n",
       "      <th>F14_spread</th>\n",
       "      <th>G14</th>\n",
       "      <th>G14_spread</th>\n",
       "      <th>H14</th>\n",
       "      <th>H14_spread</th>\n",
       "      <th>J14</th>\n",
       "      <th>J14_spread</th>\n",
       "      <th>K14</th>\n",
       "      <th>K14_spread</th>\n",
       "      <th>...</th>\n",
       "      <th>Z24</th>\n",
       "      <th>Z24_spread</th>\n",
       "      <th>F25</th>\n",
       "      <th>F25_spread</th>\n",
       "      <th>G25</th>\n",
       "      <th>G25_spread</th>\n",
       "      <th>K25</th>\n",
       "      <th>K25_spread</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q25_spread</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-02 00:00:00</th>\n",
       "      <td>112,91</td>\n",
       "      <td>-0,0169</td>\n",
       "      <td>111,63</td>\n",
       "      <td>-0,028</td>\n",
       "      <td>110,13</td>\n",
       "      <td>-0,0411</td>\n",
       "      <td>109,1</td>\n",
       "      <td>-0,0501</td>\n",
       "      <td>109,41</td>\n",
       "      <td>-0,0474</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03 00:00:00</th>\n",
       "      <td>113,19</td>\n",
       "      <td>-0,0158</td>\n",
       "      <td>111,8</td>\n",
       "      <td>-0,0279</td>\n",
       "      <td>110,24</td>\n",
       "      <td>-0,0415</td>\n",
       "      <td>109,2</td>\n",
       "      <td>-0,0505</td>\n",
       "      <td>109,48</td>\n",
       "      <td>-0,0481</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-06 00:00:00</th>\n",
       "      <td>113,6</td>\n",
       "      <td>-0,008</td>\n",
       "      <td>112,2</td>\n",
       "      <td>-0,0203</td>\n",
       "      <td>110,6</td>\n",
       "      <td>-0,0342</td>\n",
       "      <td>109,65</td>\n",
       "      <td>-0,0425</td>\n",
       "      <td>109,65</td>\n",
       "      <td>-0,0425</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-07 00:00:00</th>\n",
       "      <td>112,94</td>\n",
       "      <td>-0,0158</td>\n",
       "      <td>111,5</td>\n",
       "      <td>-0,0283</td>\n",
       "      <td>109,93</td>\n",
       "      <td>-0,042</td>\n",
       "      <td>108,92</td>\n",
       "      <td>-0,0508</td>\n",
       "      <td>108,8</td>\n",
       "      <td>-0,0519</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-08 00:00:00</th>\n",
       "      <td>112,8</td>\n",
       "      <td>-0,0172</td>\n",
       "      <td>111,59</td>\n",
       "      <td>-0,0277</td>\n",
       "      <td>109,9</td>\n",
       "      <td>-0,0424</td>\n",
       "      <td>108,99</td>\n",
       "      <td>-0,0504</td>\n",
       "      <td>108,96</td>\n",
       "      <td>-0,0506</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        F14 F14_spread     G14 G14_spread     H14 H14_spread  \\\n",
       "Data                                                                           \n",
       "2014-01-02 00:00:00  112,91    -0,0169  111,63     -0,028  110,13    -0,0411   \n",
       "2014-01-03 00:00:00  113,19    -0,0158   111,8    -0,0279  110,24    -0,0415   \n",
       "2014-01-06 00:00:00   113,6     -0,008   112,2    -0,0203   110,6    -0,0342   \n",
       "2014-01-07 00:00:00  112,94    -0,0158   111,5    -0,0283  109,93     -0,042   \n",
       "2014-01-08 00:00:00   112,8    -0,0172  111,59    -0,0277   109,9    -0,0424   \n",
       "\n",
       "                        J14 J14_spread     K14 K14_spread  ...  Z24  \\\n",
       "Data                                                       ...        \n",
       "2014-01-02 00:00:00   109,1    -0,0501  109,41    -0,0474  ...  NaN   \n",
       "2014-01-03 00:00:00   109,2    -0,0505  109,48    -0,0481  ...  NaN   \n",
       "2014-01-06 00:00:00  109,65    -0,0425  109,65    -0,0425  ...  NaN   \n",
       "2014-01-07 00:00:00  108,92    -0,0508   108,8    -0,0519  ...  NaN   \n",
       "2014-01-08 00:00:00  108,99    -0,0504  108,96    -0,0506  ...  NaN   \n",
       "\n",
       "                    Z24_spread  F25 F25_spread  G25 G25_spread  K25  \\\n",
       "Data                                                                  \n",
       "2014-01-02 00:00:00        NaN  NaN        NaN  NaN        NaN  NaN   \n",
       "2014-01-03 00:00:00        NaN  NaN        NaN  NaN        NaN  NaN   \n",
       "2014-01-06 00:00:00        NaN  NaN        NaN  NaN        NaN  NaN   \n",
       "2014-01-07 00:00:00        NaN  NaN        NaN  NaN        NaN  NaN   \n",
       "2014-01-08 00:00:00        NaN  NaN        NaN  NaN        NaN  NaN   \n",
       "\n",
       "                    K25_spread  Q25 Q25_spread  \n",
       "Data                                            \n",
       "2014-01-02 00:00:00        NaN  NaN        NaN  \n",
       "2014-01-03 00:00:00        NaN  NaN        NaN  \n",
       "2014-01-06 00:00:00        NaN  NaN        NaN  \n",
       "2014-01-07 00:00:00        NaN  NaN        NaN  \n",
       "2014-01-08 00:00:00        NaN  NaN        NaN  \n",
       "\n",
       "[5 rows x 272 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"data/00--raw/future_price.csv\"\n",
    "\n",
    "y = pd.read_csv(data, index_col=0, parse_dates=True)\n",
    "y.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3bb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] Loaded 2753 rows × 272 cols from data/00--raw/future_price.csv\n",
      "[meta] Contract-mapped columns: 136\n",
      "[long] Tidy rows: 23477\n",
      "[run] Using metric='price'\n",
      "[save] CSVs saved under data/10--derived/futures-analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64117/7828156.py:201: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  long = y2.stack([\"target_month\", \"variable\"]).rename(\"value\").reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[plot_horizon] Saved to data/10--derived/futures-analysis/plot_price_by_horizon.png\n",
      "[plot_by_month] Saved to data/10--derived/futures-analysis/plot_price_by_month.png\n",
      "[analyze_dispersion] Normalized by daily nearby (ratio).\n",
      "[analyze_dispersion] Winsorized per horizon at (0.01, 0.99).\n",
      "[dispersion] Summary -> data/10--derived/futures-analysis/dispersion_price_nearby.csv\n",
      "[plot_cv] Saved -> data/10--derived/futures-analysis/plot_cv_price_nearby.png\n",
      "[plot_cv] Saved -> data/10--derived/futures-analysis/plot_rcv_price_nearby.png\n",
      "[plot_median_iqr] Saved -> data/10--derived/futures-analysis/plot_median_iqr_price_nearby.png\n",
      "[run] Done.\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, List, Optional, Tuple, Literal\n",
    "\n",
    "import re\n",
    "import calendar\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ===================== Config & Results =====================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"End-to-end configuration for futures processing and dispersion.\"\"\"\n",
    "    csv_path: str\n",
    "    output_dir: Optional[str] = None\n",
    "    spread_regex: str = r\"(?i)\\b(spread|sprd|basis|diff|dif|bid-?ask)\\b\"\n",
    "    two_digit_year_cutoff: int = 70\n",
    "    trim_quantiles: Optional[Tuple[float, float]] = (0.01, 0.99)\n",
    "    dispersion_metric: Optional[Literal[\"price\", \"spread\"]] = None\n",
    "    dispersion_normalize: Literal[\"none\", \"nearby\", \"daily_median\"] = \"none\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Results:\n",
    "    \"\"\"Tidy data, summaries, and plot paths.\"\"\"\n",
    "    long_df: pd.DataFrame\n",
    "    summary_horizon: pd.DataFrame\n",
    "    summary_by_month: pd.DataFrame\n",
    "    plot_paths: Dict[str, Optional[str]]\n",
    "    saved_paths: Dict[str, Optional[str]]\n",
    "\n",
    "\n",
    "# ===================== Core =====================\n",
    "\n",
    "_FUTS_CODE_TO_MONTH: Dict[str, int] = {\n",
    "    \"F\": 1, \"G\": 2, \"H\": 3, \"J\": 4, \"K\": 5, \"M\": 6,\n",
    "    \"N\": 7, \"Q\": 8, \"U\": 9, \"V\": 10, \"X\": 11, \"Z\": 12,\n",
    "}\n",
    "\n",
    "\n",
    "def run(cfg: Config) -> Results:\n",
    "    \"\"\"Load, tidy, summarize, plot, and dispersion-analysis.\"\"\"\n",
    "    y = _load_and_coerce(cfg.csv_path)\n",
    "\n",
    "    meta = _build_meta(\n",
    "        y.columns,\n",
    "        spread_regex=cfg.spread_regex,\n",
    "        two_digit_year_cutoff=cfg.two_digit_year_cutoff,\n",
    "    )\n",
    "\n",
    "    long_df = _to_long(y, meta)\n",
    "\n",
    "    metric = cfg.dispersion_metric or _choose_metric(long_df)\n",
    "    print(f\"[run] Using metric='{metric}'\")\n",
    "\n",
    "    horizon = _summarize(long_df[long_df[\"variable\"] == metric], [\"horizon_months\"])\n",
    "    by_month = _summarize(long_df[long_df[\"variable\"] == metric], [\"target_month_of_year\"])\n",
    "\n",
    "    saved_paths = _save_outputs(\n",
    "        long_df=long_df,\n",
    "        horizon=horizon,\n",
    "        by_month=by_month,\n",
    "        metric=metric,\n",
    "        output_dir=cfg.output_dir,\n",
    "    )\n",
    "\n",
    "    plot_paths: Dict[str, Optional[str]] = {}\n",
    "    plot_paths[\"by_horizon\"] = _plot_horizon(horizon, cfg.output_dir, metric)\n",
    "    plot_paths[\"by_month\"] = _plot_by_month(by_month, cfg.output_dir, metric)\n",
    "\n",
    "    disp_paths = analyze_dispersion(\n",
    "        long_df=long_df,\n",
    "        metric=metric,\n",
    "        normalize=cfg.dispersion_normalize,\n",
    "        trim_quantiles=cfg.trim_quantiles,\n",
    "        output_dir=cfg.output_dir,\n",
    "    )\n",
    "    plot_paths.update(disp_paths)\n",
    "\n",
    "    print(\"[run] Done.\")\n",
    "    return Results(long_df, horizon, by_month, plot_paths, saved_paths)\n",
    "\n",
    "\n",
    "# ===================== IO & Parsing =====================\n",
    "\n",
    "def _load_and_coerce(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV (date index) and coerce numerics with pt-BR handling.\"\"\"\n",
    "    p = Path(csv_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"CSV not found at {p}\")\n",
    "\n",
    "    df = pd.read_csv(p, index_col=0, parse_dates=True, low_memory=False).sort_index()\n",
    "\n",
    "    df = df.apply(_coerce_brazilian_numeric, axis=0)\n",
    "    print(f\"[load] Loaded {df.shape[0]} rows × {df.shape[1]} cols from {p}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def _coerce_brazilian_numeric(col: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convert strings with decimal comma to floats.\"\"\"\n",
    "    if pd.api.types.is_numeric_dtype(col):\n",
    "        return col.astype(float)\n",
    "\n",
    "    s = col.astype(\"string\").str.strip()\n",
    "    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"None\": np.nan})\n",
    "\n",
    "    both = s.str.contains(r\"\\d\\.\\d\", na=False) & s.str.contains(\",\", na=False)\n",
    "    comma = s.str.contains(\",\", na=False) & ~both\n",
    "\n",
    "    s = s.where(~both, s.str.replace(\".\", \"\", regex=False))\n",
    "    s = s.where(~both, s.str.replace(\",\", \".\", regex=False))\n",
    "    s = s.where(~comma, s.str.replace(\",\", \".\", regex=False))\n",
    "    s = s.str.replace(\" \", \"\", regex=False)\n",
    "\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def _build_meta(\n",
    "    cols: Iterable[str],\n",
    "    spread_regex: str,\n",
    "    two_digit_year_cutoff: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Detect column delivery month and variable kind.\"\"\"\n",
    "    pat_spread = re.compile(spread_regex)\n",
    "    rows: List[Tuple[str, Optional[pd.Timestamp], str]] = []\n",
    "\n",
    "    for c in cols:\n",
    "        tm = _parse_contract_month(c, two_digit_year_cutoff)\n",
    "        if tm is None:\n",
    "            continue\n",
    "        var = \"spread\" if pat_spread.search(c) else \"price\"\n",
    "        rows.append((c, tm, var))\n",
    "\n",
    "    meta = pd.DataFrame(rows, columns=[\"orig_col\", \"target_month\", \"variable\"])\n",
    "    if meta.empty:\n",
    "        raise ValueError(\"No columns with recognizable (month, year) found.\")\n",
    "\n",
    "    print(f\"[meta] Contract-mapped columns: {meta.shape[0]}\")\n",
    "    return meta\n",
    "\n",
    "\n",
    "def _parse_contract_month(col: str, cutoff: int) -> Optional[pd.Timestamp]:\n",
    "    \"\"\"Parse delivery month from a column name.\"\"\"\n",
    "    s = col.strip()\n",
    "    slow = s.lower()\n",
    "\n",
    "    m = re.search(r\"(?P<y>\\d{4})[-_/\\.]?(?P<m>\\d{1,2})\\b\", s)\n",
    "    if m:\n",
    "        y4 = int(m.group(\"y\")); mm = int(m.group(\"m\"))\n",
    "        if 1 <= mm <= 12:\n",
    "            return pd.Timestamp(y4, mm, 1)\n",
    "\n",
    "    month_names = (\n",
    "        \"jan|january|feb|february|mar|march|apr|april|may|jun|june|jul|july|\"\n",
    "        \"aug|august|sep|sept|september|oct|october|nov|november|dec|december\"\n",
    "    )\n",
    "    m = re.search(rf\"(?P<mon>{month_names})\\D{{0,3}}(?P<yr>\\d{{2,4}})\\b\", slow)\n",
    "    if m:\n",
    "        mon_map = {\n",
    "            \"jan\": 1, \"january\": 1, \"feb\": 2, \"february\": 2,\n",
    "            \"mar\": 3, \"march\": 3, \"apr\": 4, \"april\": 4, \"may\": 5,\n",
    "            \"jun\": 6, \"june\": 6, \"jul\": 7, \"july\": 7, \"aug\": 8, \"august\": 8,\n",
    "            \"sep\": 9, \"sept\": 9, \"september\": 9, \"oct\": 10, \"october\": 10,\n",
    "            \"nov\": 11, \"november\": 11, \"dec\": 12, \"december\": 12,\n",
    "        }\n",
    "        mm = mon_map[m.group(\"mon\")]\n",
    "        yr = int(m.group(\"yr\"))\n",
    "        if yr < 100:\n",
    "            yr = 2000 + yr if yr < cutoff else 1900 + yr\n",
    "        return pd.Timestamp(yr, mm, 1)\n",
    "\n",
    "    m = re.search(r\"\\b([FGHJKMNQUVXZ])\\s*([0-9]{2,4})\\b\", s.upper())\n",
    "    if m:\n",
    "        code = m.group(1)\n",
    "        yr = int(m.group(2))\n",
    "        if yr < 100:\n",
    "            yr = 2000 + yr if yr < cutoff else 1900 + yr\n",
    "        mm = _FUTS_CODE_TO_MONTH.get(code)\n",
    "        if mm:\n",
    "            return pd.Timestamp(yr, mm, 1)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# ===================== Reshape & Summaries =====================\n",
    "\n",
    "def _to_long(y: pd.DataFrame, meta: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Tidy long: trade_date, target_month, horizon, variable, value.\"\"\"\n",
    "    y2 = y.loc[:, meta[\"orig_col\"]].copy()\n",
    "    y2.index.name = \"trade_date\"\n",
    "\n",
    "    y2.columns = pd.MultiIndex.from_frame(meta[[\"target_month\", \"variable\"]])\n",
    "    y2.columns.set_names([\"target_month\", \"variable\"], inplace=True)\n",
    "\n",
    "    long = y2.stack([\"target_month\", \"variable\"]).rename(\"value\").reset_index()\n",
    "\n",
    "    td = pd.to_datetime(long[\"trade_date\"]).dt.normalize()\n",
    "    tm = pd.to_datetime(long[\"target_month\"]).dt.normalize()\n",
    "\n",
    "    long[\"horizon_days\"] = (tm - td).dt.days\n",
    "    long[\"horizon_months\"] = (tm.dt.year - td.dt.year) * 12 + (tm.dt.month - td.dt.month)\n",
    "    long[\"target_month_of_year\"] = tm.dt.month\n",
    "\n",
    "    long = long.loc[long[\"horizon_days\"] >= 0].reset_index(drop=True)\n",
    "\n",
    "    cols = [\n",
    "        \"trade_date\", \"target_month\", \"target_month_of_year\",\n",
    "        \"horizon_days\", \"horizon_months\", \"variable\", \"value\",\n",
    "    ]\n",
    "    long = long[cols]\n",
    "    print(f\"[long] Tidy rows: {long.shape[0]}\")\n",
    "    return long\n",
    "\n",
    "\n",
    "def _choose_metric(long_df: pd.DataFrame) -> str:\n",
    "    \"\"\"Pick 'spread' if present with data; else 'price'.\"\"\"\n",
    "    has_spread = (long_df[\"variable\"] == \"spread\").any() and long_df.loc[long_df[\"variable\"] == \"spread\", \"value\"].notna().any()\n",
    "    return \"spread\" if has_spread else \"price\"\n",
    "\n",
    "\n",
    "def _summarize(df: pd.DataFrame, by: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Mean, std, count, q25, q75 grouped by given keys.\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=by + [\"mean\", \"std\", \"count\", \"q25\", \"q75\"])\n",
    "\n",
    "    g = df.groupby(by, observed=True)[\"value\"]\n",
    "    out = g.agg(\n",
    "        mean=\"mean\",\n",
    "        std=\"std\",\n",
    "        count=\"count\",\n",
    "        q25=lambda x: x.quantile(0.25),\n",
    "        q75=lambda x: x.quantile(0.75),\n",
    "    ).reset_index()\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===================== Output =====================\n",
    "\n",
    "def _ensure_outdir(output_dir: Optional[str]) -> Optional[Path]:\n",
    "    if output_dir is None:\n",
    "        return None\n",
    "    p = Path(output_dir)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "\n",
    "def _save_outputs(\n",
    "    long_df: pd.DataFrame,\n",
    "    horizon: pd.DataFrame,\n",
    "    by_month: pd.DataFrame,\n",
    "    metric: str,\n",
    "    output_dir: Optional[str],\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    \"\"\"Save CSVs. File names reflect the chosen metric.\"\"\"\n",
    "    outdir = _ensure_outdir(output_dir)\n",
    "    paths: Dict[str, Optional[str]] = {\"long_csv\": None, \"summary_horizon\": None, \"summary_by_month\": None}\n",
    "    if outdir is None:\n",
    "        return paths\n",
    "\n",
    "    paths[\"long_csv\"] = str(outdir / \"futures_long_tidy.csv\")\n",
    "    paths[\"summary_horizon\"] = str(outdir / f\"summary_horizon_{metric}.csv\")\n",
    "    paths[\"summary_by_month\"] = str(outdir / f\"summary_month_{metric}.csv\")\n",
    "\n",
    "    long_df.to_csv(paths[\"long_csv\"], index=False)\n",
    "    horizon.to_csv(paths[\"summary_horizon\"], index=False)\n",
    "    by_month.to_csv(paths[\"summary_by_month\"], index=False)\n",
    "\n",
    "    print(f\"[save] CSVs saved under {outdir}\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "def _plot_horizon(horizon: pd.DataFrame, output_dir: Optional[str], metric: str) -> Optional[str]:\n",
    "    \"\"\"Errorbar mean±std vs horizon months.\"\"\"\n",
    "    if horizon.empty:\n",
    "        print(\"[plot_horizon] No data to plot.\")\n",
    "        return None\n",
    "\n",
    "    hs = horizon.dropna(subset=[\"mean\"]).sort_values(\"horizon_months\")\n",
    "    if hs.empty:\n",
    "        print(\"[plot_horizon] No non-NaN means to plot.\")\n",
    "        return None\n",
    "\n",
    "    plt.figure()\n",
    "    plt.errorbar(hs[\"horizon_months\"], hs[\"mean\"], yerr=hs[\"std\"], fmt=\"-o\")\n",
    "    plt.xlabel(\"Horizon (months)\")\n",
    "    plt.ylabel(f\"{metric.capitalize()} (mean ± std)\")\n",
    "    plt.title(f\"{metric.capitalize()} by horizon\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    outdir = _ensure_outdir(output_dir)\n",
    "    path = None\n",
    "    if outdir:\n",
    "        path = str(outdir / f\"plot_{metric}_by_horizon.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path, dpi=160)\n",
    "        print(f\"[plot_horizon] Saved to {path}\")\n",
    "\n",
    "    plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "def _plot_by_month(by_month: pd.DataFrame, output_dir: Optional[str], metric: str) -> Optional[str]:\n",
    "    \"\"\"Errorbar mean±std vs delivery month with aliases on x-axis.\"\"\"\n",
    "    if by_month.empty:\n",
    "        print(\"[plot_by_month] No data to plot.\")\n",
    "        return None\n",
    "\n",
    "    md = by_month.dropna(subset=[\"mean\"]).sort_values(\"target_month_of_year\")\n",
    "    if md.empty:\n",
    "        print(\"[plot_by_month] No non-NaN means to plot.\")\n",
    "        return None\n",
    "\n",
    "    month_labels = [calendar.month_abbr[i].upper() for i in range(1, 13)]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.errorbar(md[\"target_month_of_year\"], md[\"mean\"], yerr=md[\"std\"], fmt=\"-o\")\n",
    "    plt.xticks(ticks=np.arange(1, 13, 1), labels=month_labels)\n",
    "    plt.xlabel(\"Delivery month\")\n",
    "    plt.ylabel(f\"{metric.capitalize()} (mean ± std)\")\n",
    "    plt.title(f\"{metric.capitalize()} by delivery month\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    outdir = _ensure_outdir(output_dir)\n",
    "    path = None\n",
    "    if outdir:\n",
    "        path = str(outdir / f\"plot_{metric}_by_month.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path, dpi=160)\n",
    "        print(f\"[plot_by_month] Saved to {path}\")\n",
    "\n",
    "    plt.close()\n",
    "    return path\n",
    "\n",
    "\n",
    "# ===================== Dispersion (scale-free) =====================\n",
    "\n",
    "def analyze_dispersion(\n",
    "    long_df: pd.DataFrame,\n",
    "    metric: Literal[\"price\", \"spread\"] = \"price\",\n",
    "    normalize: Literal[\"none\", \"nearby\", \"daily_median\"] = \"none\",\n",
    "    trim_quantiles: Optional[Tuple[float, float]] = (0.01, 0.99),\n",
    "    output_dir: Optional[str] = None,\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    \"\"\"Compute CV, robust CV, IQR ribbon and plot them.\"\"\"\n",
    "    df = long_df.loc[long_df[\"variable\"] == metric, [\"trade_date\", \"horizon_months\", \"value\"]].copy()\n",
    "    if df.empty:\n",
    "        print(f\"[analyze_dispersion] No rows for metric='{metric}'.\")\n",
    "        return {\"summary_csv\": None, \"cv_plot\": None, \"robust_cv_plot\": None, \"iqr_plot\": None}\n",
    "\n",
    "    df[\"trade_date\"] = pd.to_datetime(df[\"trade_date\"]).dt.normalize()\n",
    "\n",
    "    if normalize == \"nearby\":\n",
    "        base = (\n",
    "            df.loc[df.groupby(\"trade_date\")[\"horizon_months\"].transform(\"min\") == df[\"horizon_months\"],\n",
    "                   [\"trade_date\", \"value\"]]\n",
    "            .rename(columns={\"value\": \"base_value\"})\n",
    "        )\n",
    "        df = df.merge(base, on=\"trade_date\", how=\"left\")\n",
    "        df[\"value\"] = df[\"value\"] / df[\"base_value\"]\n",
    "        print(\"[analyze_dispersion] Normalized by daily nearby (ratio).\")\n",
    "\n",
    "    elif normalize == \"daily_median\":\n",
    "        med = df.groupby(\"trade_date\")[\"value\"].transform(\"median\")\n",
    "        df[\"value\"] = df[\"value\"] / med\n",
    "        print(\"[analyze_dispersion] Normalized by daily median (ratio).\")\n",
    "\n",
    "    if trim_quantiles is not None:\n",
    "        ql, qh = trim_quantiles\n",
    "        trims = df.groupby(\"horizon_months\")[\"value\"].quantile([ql, qh]).unstack()\n",
    "        df = df.join(trims, on=\"horizon_months\", rsuffix=\"_trim\")\n",
    "        df[\"value\"] = df[\"value\"].clip(lower=df[ql], upper=df[qh])\n",
    "        df.drop(columns=[ql, qh], inplace=True)\n",
    "        print(f\"[analyze_dispersion] Winsorized per horizon at {trim_quantiles}.\")\n",
    "\n",
    "    summary = _dispersion_summary(df)\n",
    "\n",
    "    return _save_and_plot_dispersion(summary, metric, normalize, output_dir)\n",
    "\n",
    "\n",
    "def _dispersion_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Mean, std, CV, robust CV, median, IQR by horizon.\"\"\"\n",
    "    g = df.groupby(\"horizon_months\")[\"value\"]\n",
    "\n",
    "    median = g.median()\n",
    "    q25 = g.quantile(0.25)\n",
    "    q75 = g.quantile(0.75)\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    mad = g.apply(lambda x: (x - x.median()).abs().median())\n",
    "    madn = 1.4826 * mad\n",
    "\n",
    "    mean = g.mean()\n",
    "    std = g.std(ddof=1)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"horizon_months\": mean.index,\n",
    "        \"mean\": mean.values,\n",
    "        \"std\": std.values,\n",
    "        \"cv\": (std / mean.replace(0, np.nan)).values,\n",
    "        \"median\": median.values,\n",
    "        \"madn\": madn.values,\n",
    "        \"rcv\": (madn / median.replace(0, np.nan)).values,\n",
    "        \"q25\": q25.values,\n",
    "        \"q75\": q75.values,\n",
    "        \"iqr\": iqr.values,\n",
    "        \"iqr_over_median\": (iqr / median.replace(0, np.nan)).values,\n",
    "        \"count\": g.size().values,\n",
    "    }).sort_values(\"horizon_months\").reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _save_and_plot_dispersion(\n",
    "    summary: pd.DataFrame,\n",
    "    metric: str,\n",
    "    normalize: str,\n",
    "    output_dir: Optional[str],\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    \"\"\"Emit CSV and dispersion plots.\"\"\"\n",
    "    pdir = _ensure_outdir(output_dir)\n",
    "    tag = f\"{metric}_{normalize}\"\n",
    "\n",
    "    csv_path = str(pdir / f\"dispersion_{tag}.csv\") if pdir else None\n",
    "    if csv_path:\n",
    "        summary.to_csv(csv_path, index=False)\n",
    "        print(f\"[dispersion] Summary -> {csv_path}\")\n",
    "\n",
    "    cv_path = _plot_cv(summary, f\"{metric.capitalize()} CV by horizon ({normalize})\", \"cv\",\n",
    "                       str(pdir / f\"plot_cv_{tag}.png\") if pdir else None)\n",
    "\n",
    "    rcv_path = _plot_cv(summary, f\"{metric.capitalize()} robust CV (MADn/median) by horizon ({normalize})\", \"rcv\",\n",
    "                        str(pdir / f\"plot_rcv_{tag}.png\") if pdir else None)\n",
    "\n",
    "    iqr_path = _plot_median_iqr(summary, metric, normalize,\n",
    "                                str(pdir / f\"plot_median_iqr_{tag}.png\") if pdir else None)\n",
    "\n",
    "    return {\"summary_csv\": csv_path, \"cv_plot\": cv_path, \"robust_cv_plot\": rcv_path, \"iqr_plot\": iqr_path}\n",
    "\n",
    "\n",
    "def _plot_cv(summary: pd.DataFrame, title: str, y_col: str, output_path: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Plot CV-like measure vs horizon.\"\"\"\n",
    "    if summary.empty or summary[y_col].isna().all():\n",
    "        print(f\"[plot_cv] Nothing to plot for {y_col}.\")\n",
    "        return None\n",
    "\n",
    "    s = summary.dropna(subset=[y_col]).sort_values(\"horizon_months\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(s[\"horizon_months\"], s[y_col], marker=\"o\")\n",
    "    plt.xlabel(\"Horizon (months)\")\n",
    "    plt.ylabel(y_col.upper())\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "\n",
    "    if output_path:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=160)\n",
    "        print(f\"[plot_cv] Saved -> {output_path}\")\n",
    "\n",
    "    plt.close()\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def _plot_median_iqr(summary: pd.DataFrame, metric: str, normalize: str, output_path: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Median with IQR ribbon vs horizon.\"\"\"\n",
    "    if summary.empty:\n",
    "        print(\"[plot_median_iqr] Nothing to plot.\")\n",
    "        return None\n",
    "\n",
    "    s = summary.sort_values(\"horizon_months\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(s[\"horizon_months\"], s[\"median\"], marker=\"o\")\n",
    "    plt.fill_between(s[\"horizon_months\"], s[\"q25\"], s[\"q75\"], alpha=0.25, label=\"IQR\")\n",
    "    plt.xlabel(\"Horizon (months)\")\n",
    "    plt.ylabel(f\"{metric.capitalize()} (median & IQR)\")\n",
    "    plt.title(f\"{metric.capitalize()} median with IQR by horizon ({normalize})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    if output_path:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=160)\n",
    "        print(f\"[plot_median_iqr] Saved -> {output_path}\")\n",
    "\n",
    "    plt.close()\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# ===================== Example =====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Config(\n",
    "        csv_path=\"data/00--raw/future_price.csv\",\n",
    "        output_dir=\"data/10--derived/futures-analysis\",\n",
    "        dispersion_normalize=\"nearby\"  # try \"none\" | \"nearby\" | \"daily_median\"\n",
    "    )\n",
    "    _ = run(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8937bbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading actuals: data/00--raw/price.csv\n",
      "Detected actual columns -> B3: Boi gordo B3 | Cepea: Boi gordo Cepea\n",
      "Reading futures: data/00--raw/future_price.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64117/2861966300.py:134: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  fut = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 136 contract columns (e.g., first 10): ['F14', 'G14', 'H14', 'J14', 'K14', 'M14', 'N14', 'Q14', 'U14', 'V14']\n",
      "Wrote data/10--derived/prediction-aware/plot_date_aligned_3m.png\n",
      "Wrote data/10--derived/prediction-aware/plot_date_aligned_6m.png\n",
      "Wrote data/10--derived/prediction-aware/plot_date_aligned_12m.png\n",
      "Wrote data/10--derived/prediction-aware/plot_abs_error_violins.png\n",
      "Wrote data/10--derived/prediction-aware/prediction_error_summary.csv\n",
      "Outputs: {'3m_plot': 'data/10--derived/prediction-aware/plot_date_aligned_3m.png', '6m_plot': 'data/10--derived/prediction-aware/plot_date_aligned_6m.png', '12m_plot': 'data/10--derived/prediction-aware/plot_date_aligned_12m.png', 'violins': 'data/10--derived/prediction-aware/plot_abs_error_violins.png', 'summary_csv': 'data/10--derived/prediction-aware/prediction_error_summary.csv'}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Prediction-aware alignment for Boi Gordo futures vs. actuals (B3 & Cepea).\n",
    "\n",
    "Inputs\n",
    "------\n",
    "ACTUALS_CSV = \"data/00--raw/price.csv\"\n",
    "  Columns (pt-BR format, example):\n",
    "    Data, Boi gordo B3, Boi gordo Cepea, Spread Absoluto, Spread Relativo\n",
    "  Dates like \"01.09.2020\"; numbers like \"242,4\".\n",
    "\n",
    "PREDICTIONS_CSV = \"data/00--raw/future-price.csv\"\n",
    "  Wide panel with one row per issue date and many contract columns:\n",
    "    Data, F14, F14_spread, G14, G14_spread, H14, ... , Z25, Z25_spread\n",
    "  Dates like \"2014-01-02 00:00:00\"; numbers like \"112,91\".\n",
    "\n",
    "Behavior\n",
    "--------\n",
    "For each issue date d and horizon h ∈ {3,6,12} months:\n",
    "- target_date = d + h months\n",
    "- compute the contract code for (target_date.year, target_date.month), e.g. Mar/2014 -> \"H14\"\n",
    "- y_pred = futures[d, code]\n",
    "- y_actual = actuals[target_date] (for B3 and Cepea; daily ffilled)\n",
    "\n",
    "Outputs (written to OUT_DIR = \"data/10--derived/prediction_aware\")\n",
    "------------------------------------------------------------------\n",
    "- plot_date_aligned_3m.png\n",
    "- plot_date_aligned_6m.png\n",
    "- plot_date_aligned_12m.png\n",
    "- plot_abs_error_violins.png\n",
    "- prediction_error_summary.csv\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- Locale-aware numeric parsing (pt-BR).\n",
    "- Actuals are resampled to daily and forward-filled.\n",
    "- Uses matplotlib only (no seaborn), one chart per figure, no explicit colors.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import AutoDateLocator, ConciseDateFormatter\n",
    "\n",
    "# ----------------------\n",
    "# Config (edit as needed)\n",
    "# ----------------------\n",
    "ACTUALS_CSV = \"data/00--raw/price.csv\"\n",
    "PREDICTIONS_CSV = \"data/00--raw/future_price.csv\"\n",
    "OUT_DIR = \"data/10--derived/prediction-aware\"\n",
    "HORIZONS = [3, 6, 12]  # months\n",
    "\n",
    "# Month code map used by futures:\n",
    "# F=Jan(1), G=Feb(2), H=Mar(3), J=Apr(4), K=May(5), M=Jun(6),\n",
    "# N=Jul(7), Q=Aug(8), U=Sep(9), V=Oct(10), X=Nov(11), Z=Dec(12)\n",
    "MONTH_TO_CODE = {1: 'F', 2: 'G', 3: 'H', 4: 'J', 5: 'K', 6: 'M',\n",
    "                 7: 'N', 8: 'Q', 9: 'U', 10: 'V', 11: 'X', 12: 'Z'}\n",
    "CODE_RE = re.compile(r'^[FGHJKMNQUVXZ]\\d{2}$')  # e.g., F14, H20, Z25\n",
    "\n",
    "\n",
    "# ---------\n",
    "# Utilities\n",
    "# ---------\n",
    "def _coerce_numeric_ptbr(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Parse strings like '2.345,67' -> 2345.67. Leaves numeric dtypes unchanged.\"\"\"\n",
    "    if s.dtype.kind in \"iufc\":\n",
    "        return s\n",
    "    s2 = s.astype(str).str.strip()\n",
    "    has_comma = s2.str.contains(\",\", regex=False, na=False)\n",
    "    s2a = s2.where(~has_comma,\n",
    "                   s2.str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False))\n",
    "    return pd.to_numeric(s2a, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def _ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Load & prep actuals\n",
    "# -------------------\n",
    "def _read_actuals(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    # Date column is \"Data\" with dd.mm.yyyy pattern (pt-BR)\n",
    "    date_col = None\n",
    "    if 'Data' in df.columns:\n",
    "        date_col = 'Data'\n",
    "    else:\n",
    "        # fallback scan\n",
    "        for c in df.columns:\n",
    "            try:\n",
    "                parsed = pd.to_datetime(df[c], errors='coerce', dayfirst=True)\n",
    "                if parsed.notna().mean() > 0.6:\n",
    "                    date_col = c\n",
    "                    break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if date_col is None:\n",
    "        raise ValueError(\"Could not find a date column in actuals (expected 'Data').\")\n",
    "\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True)\n",
    "    df = df.dropna(subset=[date_col]).sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "    # Identify B3 / Cepea columns by name\n",
    "    def _find_col(name_part: str):\n",
    "        for c in df.columns:\n",
    "            if name_part.lower() in c.lower():\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    b3_col = _find_col(\"Boi gordo B3\")\n",
    "    cepea_col = _find_col(\"Boi gordo Cepea\")\n",
    "    if b3_col is None or cepea_col is None:\n",
    "        raise ValueError(\"Could not find 'Boi gordo B3' and/or 'Boi gordo Cepea' in actuals.\")\n",
    "\n",
    "    # Coerce numeric (pt-BR to float)\n",
    "    df[b3_col] = _coerce_numeric_ptbr(df[b3_col])\n",
    "    df[cepea_col] = _coerce_numeric_ptbr(df[cepea_col])\n",
    "\n",
    "    # Build daily forward-filled index\n",
    "    adf = df[[date_col, b3_col, cepea_col]].copy().set_index(date_col).sort_index()\n",
    "    full_idx = pd.date_range(adf.index.min(), adf.index.max(), freq='D')\n",
    "    adf = adf.reindex(full_idx).ffill()\n",
    "    adf.index.name = 'date'\n",
    "    return adf, b3_col, cepea_col\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Load & prep futures wide\n",
    "# -----------------------\n",
    "def _read_futures_wide(path: str):\n",
    "    fut = pd.read_csv(path)\n",
    "    # Date column is \"Data\" with ISO + time (e.g., 2014-01-02 00:00:00)\n",
    "    if 'Data' not in fut.columns:\n",
    "        raise ValueError(\"Futures file is missing 'Data' column.\")\n",
    "    fut['Data'] = pd.to_datetime(fut['Data'], errors='coerce', dayfirst=False)\n",
    "    fut = fut.dropna(subset=['Data']).sort_values('Data').reset_index(drop=True)\n",
    "\n",
    "    # Identify pure contract columns like 'F14', 'H21', etc. (ignore *_spread)\n",
    "    contract_cols = [c for c in fut.columns if CODE_RE.match(c)]\n",
    "    # Coerce numeric pt-BR for those columns\n",
    "    for c in contract_cols:\n",
    "        fut[c] = _coerce_numeric_ptbr(fut[c])\n",
    "\n",
    "    return fut, contract_cols\n",
    "\n",
    "\n",
    "def _code_for_ym(year: int, month: int) -> str:\n",
    "    \"\"\"Return futures code like 'H14' for Mar/2014.\"\"\"\n",
    "    code = MONTH_TO_CODE[month]\n",
    "    yy = year % 100\n",
    "    return f\"{code}{yy:02d}\"\n",
    "\n",
    "\n",
    "def _build_predictions_from_futures(fut: pd.DataFrame, contract_cols: list, horizons=(3, 6, 12)):\n",
    "    \"\"\"\n",
    "    Convert wide futures panel into a long predictions table with:\n",
    "      issue_date, target_date, horizon_m, y_pred\n",
    "    For each row (issue date) and each horizon h, pick the column whose code equals (issue_date + h months).\n",
    "    \"\"\"\n",
    "    fut = fut.copy()\n",
    "    issue_dates = fut['Data']\n",
    "    out_frames = []\n",
    "    colset = set(contract_cols)\n",
    "\n",
    "    for h in horizons:\n",
    "        # Compute target dates = issue_date + h months (month-preserving)\n",
    "        target_dates = issue_dates + pd.DateOffset(months=h)\n",
    "        # Build the code string for each row's target Y-M\n",
    "        codes = [_code_for_ym(d.year, d.month) for d in target_dates]\n",
    "        # Extract predicted values row-by-row from the matching column (if present)\n",
    "        # We'll index by integer position to avoid alignment surprises.\n",
    "        preds = []\n",
    "        for i, code in enumerate(codes):\n",
    "            if code in colset:\n",
    "                preds.append(fut.at[i, code])\n",
    "            else:\n",
    "                preds.append(np.nan)\n",
    "        df_h = pd.DataFrame({\n",
    "            'issue_date': issue_dates.values,\n",
    "            'target_date': target_dates.values,\n",
    "            'horizon_m': h,\n",
    "            'y_pred': preds\n",
    "        })\n",
    "        out_frames.append(df_h)\n",
    "\n",
    "    preds_long = pd.concat(out_frames, ignore_index=True)\n",
    "    preds_long = preds_long.dropna(subset=['y_pred']).reset_index(drop=True)\n",
    "    return preds_long\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Merge with actuals & plots\n",
    "# -------------------------\n",
    "def _attach_actuals(preds: pd.DataFrame, actuals: pd.DataFrame, b3_col: str, cepea_col: str):\n",
    "    out = preds.copy()\n",
    "    out['actual_B3'] = actuals.reindex(out['target_date'])[b3_col].values\n",
    "    out['actual_Cepea'] = actuals.reindex(out['target_date'])[cepea_col].values\n",
    "    out = out.dropna(subset=['actual_B3', 'actual_Cepea']).reset_index(drop=True)\n",
    "    out['x_date'] = out['issue_date']  # align on issue date for plotting\n",
    "    return out\n",
    "\n",
    "\n",
    "def _plot_date_aligned(df_h: pd.DataFrame, horizon: int, outpath: str):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df_h['x_date'], df_h['y_pred'], label='Prediction')\n",
    "    plt.plot(df_h['x_date'], df_h['actual_B3'], label='Actual B3 (shifted)')\n",
    "    plt.plot(df_h['x_date'], df_h['actual_Cepea'], label='Actual Cepea (shifted)')\n",
    "    plt.title(f'Prediction-aware alignment: {horizon}m horizon')\n",
    "    plt.xlabel('Issue date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    ax = plt.gca()\n",
    "    locator = AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    ax.xaxis.set_major_formatter(ConciseDateFormatter(locator))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _plot_violin_abs_errors(err_long: pd.DataFrame, outpath: str):\n",
    "    horizons = sorted(err_long['horizon_m'].unique())\n",
    "    positions, datasets, medians, xticks, xticklabels = [], [], [], [], []\n",
    "    for i, h in enumerate(horizons):\n",
    "        left = i*3 + 1.0 - 0.3\n",
    "        right = i*3 + 1.0 + 0.3\n",
    "        b3 = err_long[(err_long['horizon_m']==h) & (err_long['series']=='B3')]['abs_error'].dropna().values\n",
    "        cp = err_long[(err_long['horizon_m']==h) & (err_long['series']=='Cepea')]['abs_error'].dropna().values\n",
    "        positions.extend([left, right])\n",
    "        datasets.extend([b3, cp])\n",
    "        medians.extend([np.median(b3) if len(b3)>0 else np.nan,\n",
    "                        np.median(cp) if len(cp)>0 else np.nan])\n",
    "        xticks.append(i*3 + 1.0)\n",
    "        xticklabels.append(f'{h}m')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.violinplot(datasets, positions=positions, showmeans=False, showmedians=False, showextrema=False)\n",
    "    plt.scatter(positions, medians, marker='o', zorder=3, label='Median (dot)')\n",
    "    plt.xticks(xticks, xticklabels)\n",
    "    plt.xlabel('Horizon')\n",
    "    plt.ylabel('|Error|')\n",
    "    plt.title('Absolute error by horizon (B3 vs Cepea)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.text(0.02, 0.95, 'Left = B3, Right = Cepea', transform=plt.gca().transAxes, va='top')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _summarize_errors(df: pd.DataFrame):\n",
    "    rows = []\n",
    "    for s in ['B3', 'Cepea']:\n",
    "        a = df['actual_'+s]\n",
    "        e = a - df['y_pred']\n",
    "        ae = e.abs()\n",
    "        mape = (ae / np.where(a != 0, a, np.nan)).mean()\n",
    "        rows.append({\n",
    "            'series': s,\n",
    "            'count': int(ae.notna().sum()),\n",
    "            'mae': float(ae.mean()),\n",
    "            'median_ae': float(ae.median()),\n",
    "            'rmse': float(np.sqrt((e**2).mean())),\n",
    "            'bias': float(e.mean()),\n",
    "            'mape': float(mape) if not np.isnan(mape) else np.nan,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# --------------\n",
    "# Main pipeline\n",
    "# --------------\n",
    "def run_pipeline():\n",
    "    _ensure_dir(OUT_DIR)\n",
    "\n",
    "    print(\"Reading actuals:\", ACTUALS_CSV)\n",
    "    actuals, b3_col, cepea_col = _read_actuals(ACTUALS_CSV)\n",
    "    print(\"Detected actual columns -> B3:\", b3_col, \"| Cepea:\", cepea_col)\n",
    "\n",
    "    print(\"Reading futures:\", PREDICTIONS_CSV)\n",
    "    fut, contract_cols = _read_futures_wide(PREDICTIONS_CSV)\n",
    "    print(f\"Detected {len(contract_cols)} contract columns (e.g., first 10):\", contract_cols[:10])\n",
    "\n",
    "    preds = _build_predictions_from_futures(fut, contract_cols, horizons=HORIZONS)\n",
    "    if preds.empty:\n",
    "        print(\"No predictions could be constructed for horizons:\", HORIZONS)\n",
    "        return {\"note\": \"No predictions for requested horizons.\"}\n",
    "\n",
    "    merged = _attach_actuals(preds, actuals, b3_col, cepea_col)\n",
    "    if merged.empty:\n",
    "        print(\"No rows after attaching actuals (date overlap may be empty).\")\n",
    "        return {\"note\": \"No overlap between predictions target dates and actuals.\"}\n",
    "\n",
    "    outputs = {}\n",
    "\n",
    "    # Date-aligned plots\n",
    "    for h in HORIZONS:\n",
    "        df_h = merged[merged['horizon_m'] == h].copy()\n",
    "        if df_h.empty:\n",
    "            continue\n",
    "        outpath = os.path.join(OUT_DIR, f\"plot_date_aligned_{h}m.png\")\n",
    "        _plot_date_aligned(df_h, h, outpath)\n",
    "        outputs[f\"{h}m_plot\"] = outpath\n",
    "        print(\"Wrote\", outpath)\n",
    "\n",
    "    # Violin of |error|\n",
    "    err_rows = []\n",
    "    for _, r in merged.iterrows():\n",
    "        for s in ['B3', 'Cepea']:\n",
    "            e = r[f'actual_{s}'] - r['y_pred']\n",
    "            err_rows.append({'horizon_m': int(r['horizon_m']), 'series': s, 'abs_error': abs(e)})\n",
    "    err_long = pd.DataFrame(err_rows)\n",
    "\n",
    "    violin_path = os.path.join(OUT_DIR, \"plot_abs_error_violins.png\")\n",
    "    _plot_violin_abs_errors(err_long, violin_path)\n",
    "    outputs[\"violins\"] = violin_path\n",
    "    print(\"Wrote\", violin_path)\n",
    "\n",
    "    # Summary CSV\n",
    "    summary_rows = []\n",
    "    for h in HORIZONS:\n",
    "        df_h = merged[merged['horizon_m'] == h].copy()\n",
    "        if df_h.empty:\n",
    "            continue\n",
    "        s_df = _summarize_errors(df_h)\n",
    "        s_df.insert(0, 'horizon_m', h)\n",
    "        summary_rows.append(s_df)\n",
    "    if summary_rows:\n",
    "        summary = pd.concat(summary_rows, ignore_index=True)\n",
    "        summary_path = os.path.join(OUT_DIR, \"prediction_error_summary.csv\")\n",
    "        summary.to_csv(summary_path, index=False)\n",
    "        outputs[\"summary_csv\"] = summary_path\n",
    "        print(\"Wrote\", summary_path)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    out = run_pipeline()\n",
    "    print(\"Outputs:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987b1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
